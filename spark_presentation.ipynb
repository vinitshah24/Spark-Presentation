{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Spark\n",
    "\n",
    "<img src=\"assets/spark.jpg\" width=\"300\" height=\"300\">\n",
    "\n",
    "<font size=\"3\">\n",
    "<ul>\n",
    "<li>Open-source unified analytics engine for large-scale data processing.</li>\n",
    "<li>Provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.</li>\n",
    "<li>Started as a research project at the UC Berkeley AMPLab in 2009, and was open sourced in early 2010.</li>\n",
    "</ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation\n",
    "\n",
    "## MapReduce\n",
    "<font size=\"3\">\n",
    "<p></p>\n",
    "<ul>\n",
    "<li>Scalable processing engine of the Hadoop ecosystem</li>\n",
    "<li><p><span style=\"color:#E0115F\"><b>Disk-based data processing framework (HDFS files)</b></span></p></li>\n",
    "<li><p><span style=\"color:#E0115F\"><b>Data is reloaded from disk with every query → Costly I/O</b></span></p></li> \t\n",
    "<li>Persists intermediate results to disk</li>\n",
    "<li>Costly I/O → Not appropriate for iterative or stream processing workloads</li>\n",
    "<li><p><span style=\"color:green\"><b>Best for ETL like workloads (batch processing)</b></span></p></li>\n",
    "</ul>\n",
    "</font>\n",
    "\n",
    "## Spark\n",
    "\n",
    "<font size=\"3\">\n",
    "<p></p>\n",
    "<ul>\n",
    "<li><p><span style=\"color:#588BAE\"><b>Memory based data processing framework</b></span></p></li>\n",
    "<li><p><span style=\"color:#588BAE\"><b>Avoids costly I/O by keeping intermediate results in memory</b></span></p></li>\n",
    "<li>Leverages distributed memory and remembers operations applied to dataset</li>\n",
    "<li>Data locality based computation → High Performance</li>\n",
    "<li><p><span style=\"color:green\"><b>Best for both iterative (or stream processing) and batch workloads</b></span></p></li>\n",
    "</ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Architecture\n",
    "<font size=\"3\">\n",
    "<p></p>\n",
    "<ul>\n",
    "<li>Driver program: The process running the main() function of the application and creating the SparkContext</li>\n",
    "<li>Cluster manager: An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN)</li>\n",
    "<li>Worker node: Any node that can run application code in the cluster</li>\n",
    "<li>Executor: A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them.</li>\n",
    "<li>Task: A unit of work that will be sent to one executor</li>\n",
    "</ul>    \n",
    "</font>\n",
    "<img src=\"assets/arch.png\" width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark Ecosystem\n",
    "\n",
    "<img src=\"assets/spark_ecosystem.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "### Spark RDD\n",
    "\n",
    "<font size=\"3\">\n",
    "<p></p>\n",
    "<ul>\n",
    "<li>Resilient Distributed Datasets (RDD) are the primary abstraction in Spark.</li>\n",
    "<li>Fault-tolerant collection of elements that can be operated on in parallel.</li>\n",
    "<li>2 types of operations on RDDs: <b>Transformations and Actions</b></li>\n",
    "<li>Transformations are lazy (not computed immediately)</li>\n",
    "<li>Transformed RDD gets recomputed when an action is run on it (default)</li>\n",
    "<li>RDD's can be persisted into storage in memory or disk</li>\n",
    "<li>Spark can create RDDs from any file stored in HDFS, local file system, Amazon S3, Hypertable, HBase, etc.</li>\n",
    "<p></p>\n",
    "</ul>\n",
    "</font>    \n",
    "<img src=\"assets/spark_lazy_evaluation.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "### SparkSQL\n",
    "<font size=\"3\">\n",
    "<p></p>\n",
    "<ul>\n",
    "<li>Module for working with structured and semi-structured data</li>\n",
    "<li>It originated to overcome the limitations of Apache Hive lags in performance as it uses MapReduce jobs for executing ad-hoc queries.</li>\n",
    "<li>Performs better than Hive in most scenarios</li>\n",
    "</ul>\n",
    "</font> \n",
    "\n",
    "<img src=\"assets/spark_sources.jpg\" width=\"600\" height=\"500\">\n",
    "\n",
    "### Spark MLlib\n",
    "<font size=\"3\">\n",
    "<p></p>\n",
    "<ul>\n",
    "<li>Provides tools for classification, regression, clustering, and collaborative filtering feature extraction, transformation, dimensionality reduction.</li>\n",
    "<li>Also provides selection tools for constructing, evaluating, and tuning ML pipelines.</li>\n",
    "</ul>\n",
    "</font> \n",
    "\n",
    "<img src=\"assets/spark_ml.jpg\" width=\"600\" height=\"500\">\n",
    "\n",
    "### Spark Streaming\n",
    "<font size=\"3\">\n",
    "<p></p>\n",
    "<ul>\n",
    "<li>Extends the core API to allow high-throughput, fault-tolerant stream processing of live data streams.</li>\n",
    "<li>Data can be ingested from many sources: Kafka, Flume, Twitter, ZeroMQ, TCP sockets, etc.</li>\n",
    "<li>Results can be pushed out to filesystems, databases, live dashboards, etc.</li>\n",
    "<li>Spark’s built-in machine learning algorithms and graph processing algorithms can be applied to data streams.</li>\n",
    "</ul>\n",
    "</font>\n",
    "\n",
    "<img src=\"assets/spark_streaming.png\" width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case:\n",
    "<img src=\"assets/spark_usecase.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Code\\\\Hadoop-Setup\\\\Spark-3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Lorem', 2)\n",
      "('Ipsum', 2)\n",
      "('is', 1)\n",
      "('simply', 1)\n",
      "('dummy', 2)\n",
      "('text', 2)\n",
      "('of', 2)\n",
      "('the', 3)\n",
      "('printing', 1)\n",
      "('and', 2)\n",
      "('typesetting', 1)\n",
      "('industry.', 1)\n",
      "('has', 1)\n",
      "('been', 1)\n",
      "(\"industry's\", 1)\n",
      "('standard', 1)\n",
      "('ever', 1)\n",
      "('since', 1)\n",
      "('1500s,', 1)\n",
      "('when', 1)\n",
      "('an', 1)\n",
      "('unknown', 1)\n",
      "('printer', 1)\n",
      "('took', 1)\n",
      "('a', 2)\n",
      "('galley', 1)\n",
      "('type', 2)\n",
      "('scrambled', 1)\n",
      "('it', 1)\n",
      "('to', 1)\n",
      "('make', 1)\n",
      "('specimen', 1)\n",
      "('book.', 1)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create Spark Context\n",
    "sc = SparkContext(\"local\", \"WordCount App\")\n",
    "\n",
    "# Open text file\n",
    "text_file = sc.textFile(\"data/word_count.txt\")\n",
    "\n",
    "# Transformations\n",
    "word_count = text_file \\\n",
    "            .flatMap(lambda line: line.split(\" \")) \\\n",
    "            .map(lambda word: (word, 1)) \\\n",
    "            .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Action\n",
    "result = word_count.collect()\n",
    "\n",
    "# Result\n",
    "for item in result:\n",
    "    print(item)\n",
    "\n",
    "# Save result as text file\n",
    "# result.saveAsTextFile(\"output/word_count\")\n",
    "\n",
    "# End the Spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"Titanic App\").getOrCreate()\n",
    "\n",
    "# Read CSV file\n",
    "df = spark.read \\\n",
    "        .option(\"delimiter\", \",\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(\"data/titanic.csv\")\n",
    "\n",
    "# Drop the row if all values are null\n",
    "df.na.drop(\"all\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|max(Fare)|\n",
      "+---------+\n",
      "|512.3292 |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Select Max value from Fare column\n",
    "df.select(F.max(\"Fare\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------+\n",
      "| Age|   Sex|    Fare|\n",
      "+----+------+--------+\n",
      "|38.0|female| 71.2833|\n",
      "|26.0|female|   7.925|\n",
      "|35.0|female|    53.1|\n",
      "|27.0|female| 11.1333|\n",
      "|14.0|female| 30.0708|\n",
      "| 4.0|female|    16.7|\n",
      "|58.0|female|   26.55|\n",
      "|55.0|female|    16.0|\n",
      "|null|  male|    13.0|\n",
      "|null|female|   7.225|\n",
      "|34.0|  male|    13.0|\n",
      "|15.0|female|  8.0292|\n",
      "|28.0|  male|    35.5|\n",
      "|38.0|female| 31.3875|\n",
      "|null|female|  7.8792|\n",
      "|null|female|146.5208|\n",
      "|null|female|    7.75|\n",
      "|null|  male|  7.2292|\n",
      "|14.0|female| 11.2417|\n",
      "| 3.0|female| 41.5792|\n",
      "+----+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Temp View for the dataset\n",
    "df.createOrReplaceTempView(\"titanic\")\n",
    "\n",
    "# Query the table\n",
    "spark.sql(\"SELECT Age, Sex, Fare FROM titanic WHERE Survived = 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Partition the data by Survived column and export the data in parquet file format\n",
    "df.write.partitionBy(\"Survived\").parquet(\"output/parquet\")\n",
    "\n",
    "# Partition the data by Survived column and export the data to a Hive table\n",
    "df.write.partitionBy(\"Survived\").format(\"hive\").saveAsTable(\"titanic_partition_table\")\n",
    "\n",
    "# Stop the Spark Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
